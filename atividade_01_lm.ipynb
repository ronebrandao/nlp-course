{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronebrandao/nlp-course/blob/main/atividade_01_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSbf4X_sBon2"
      },
      "source": [
        "# Processamento de Linguagem Natural (NLP)\n",
        "\n",
        "Professor: Arlindo Galvão\n",
        "\n",
        "Data: 03/09/2024\n",
        "\n",
        "## Cronograma\n",
        "\n",
        "* Parte I: N-Gram Language Model\n",
        "    \n",
        "* Parte II: Character-Level RNN Language Model\n",
        "\n",
        "## OBS\n",
        "Deixar registrado as repostas nas saídas das celulas do notebook de submissão."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alwru669_i_J"
      },
      "source": [
        "## N-Gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5_RCJnf_lmU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqmNe3Bq_n3l"
      },
      "outputs": [],
      "source": [
        "class NGramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    # Inputs são representados pelos seus indices no vocabulario\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "    def generate(self, input_sentence, max_len=128):\n",
        "        generated_sentence = input_sentence.copy()\n",
        "        for _ in range(max_len):\n",
        "            context = torch.tensor([word_to_ix[w] for w in generated_sentence[-CONTEXT_SIZE:]], dtype=torch.long)\n",
        "            log_probs = self.forward(context)\n",
        "            predicted_word_idx = torch.argmax(log_probs).item()\n",
        "            generated_sentence.append(ix_to_word[predicted_word_idx])\n",
        "        return generated_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cakVuXXnACti"
      },
      "outputs": [],
      "source": [
        "# Parâmetros do modelo\n",
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zMi51_b9qwJ",
        "outputId": "6e5f4f39-6998-4e90-b295-94adf187197f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LVqLyBr9yC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ace0d78-ab0f-40c3-d46a-36accae6ecb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege'), (['shall', 'besiege'], 'thy'), (['besiege', 'thy'], 'brow,'), (['thy', 'brow,'], 'And'), (['brow,', 'And'], 'dig'), (['And', 'dig'], 'deep'), (['dig', 'deep'], 'trenches'), (['deep', 'trenches'], 'in'), (['trenches', 'in'], 'thy'), (['in', 'thy'], \"beauty's\"), (['thy', \"beauty's\"], 'field,'), ([\"beauty's\", 'field,'], 'Thy'), (['field,', 'Thy'], \"youth's\"), (['Thy', \"youth's\"], 'proud'), ([\"youth's\", 'proud'], 'livery'), (['proud', 'livery'], 'so'), (['livery', 'so'], 'gazed'), (['so', 'gazed'], 'on'), (['gazed', 'on'], 'now,'), (['on', 'now,'], 'Will'), (['now,', 'Will'], 'be'), (['Will', 'be'], 'a'), (['be', 'a'], \"totter'd\"), (['a', \"totter'd\"], 'weed'), ([\"totter'd\", 'weed'], 'of'), (['weed', 'of'], 'small'), (['of', 'small'], 'worth'), (['small', 'worth'], 'held:'), (['worth', 'held:'], 'Then'), (['held:', 'Then'], 'being'), (['Then', 'being'], 'asked,'), (['being', 'asked,'], 'where'), (['asked,', 'where'], 'all'), (['where', 'all'], 'thy'), (['all', 'thy'], 'beauty'), (['thy', 'beauty'], 'lies,'), (['beauty', 'lies,'], 'Where'), (['lies,', 'Where'], 'all'), (['Where', 'all'], 'the'), (['all', 'the'], 'treasure'), (['the', 'treasure'], 'of'), (['treasure', 'of'], 'thy'), (['of', 'thy'], 'lusty'), (['thy', 'lusty'], 'days;'), (['lusty', 'days;'], 'To'), (['days;', 'To'], 'say,'), (['To', 'say,'], 'within'), (['say,', 'within'], 'thine'), (['within', 'thine'], 'own'), (['thine', 'own'], 'deep'), (['own', 'deep'], 'sunken'), (['deep', 'sunken'], 'eyes,'), (['sunken', 'eyes,'], 'Were'), (['eyes,', 'Were'], 'an'), (['Were', 'an'], 'all-eating'), (['an', 'all-eating'], 'shame,'), (['all-eating', 'shame,'], 'and'), (['shame,', 'and'], 'thriftless'), (['and', 'thriftless'], 'praise.'), (['thriftless', 'praise.'], 'How'), (['praise.', 'How'], 'much'), (['How', 'much'], 'more'), (['much', 'more'], 'praise'), (['more', 'praise'], \"deserv'd\"), (['praise', \"deserv'd\"], 'thy'), ([\"deserv'd\", 'thy'], \"beauty's\"), (['thy', \"beauty's\"], 'use,'), ([\"beauty's\", 'use,'], 'If'), (['use,', 'If'], 'thou'), (['If', 'thou'], 'couldst'), (['thou', 'couldst'], 'answer'), (['couldst', 'answer'], \"'This\"), (['answer', \"'This\"], 'fair'), ([\"'This\", 'fair'], 'child'), (['fair', 'child'], 'of'), (['child', 'of'], 'mine'), (['of', 'mine'], 'Shall'), (['mine', 'Shall'], 'sum'), (['Shall', 'sum'], 'my'), (['sum', 'my'], 'count,'), (['my', 'count,'], 'and'), (['count,', 'and'], 'make'), (['and', 'make'], 'my'), (['make', 'my'], 'old'), (['my', 'old'], \"excuse,'\"), (['old', \"excuse,'\"], 'Proving'), ([\"excuse,'\", 'Proving'], 'his'), (['Proving', 'his'], 'beauty'), (['his', 'beauty'], 'by'), (['beauty', 'by'], 'succession'), (['by', 'succession'], 'thine!'), (['succession', 'thine!'], 'This'), (['thine!', 'This'], 'were'), (['This', 'were'], 'to'), (['were', 'to'], 'be'), (['to', 'be'], 'new'), (['be', 'new'], 'made'), (['new', 'made'], 'when'), (['made', 'when'], 'thou'), (['when', 'thou'], 'art'), (['thou', 'art'], 'old,'), (['art', 'old,'], 'And'), (['old,', 'And'], 'see'), (['And', 'see'], 'thy'), (['see', 'thy'], 'blood'), (['thy', 'blood'], 'warm'), (['blood', 'warm'], 'when'), (['warm', 'when'], 'thou'), (['when', 'thou'], \"feel'st\"), (['thou', \"feel'st\"], 'it'), ([\"feel'st\", 'it'], 'cold.')]\n"
          ]
        }
      ],
      "source": [
        "# Criar n-grams\n",
        "ngrams = [([test_sentence[i - CONTEXT_SIZE + j] for j in range(CONTEXT_SIZE)], test_sentence[i]) for i in range(CONTEXT_SIZE, len(test_sentence))]\n",
        "print(ngrams)\n",
        "# Construir o vocabulário\n",
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for word, i in word_to_ix.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0BbXTG187mk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13dc4541-9712-47e6-c02c-c9c4d9da2bd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses: [518.3761575222015, 516.0020551681519, 513.64444231987, 511.3039126396179, 508.9805669784546, 506.67329835891724, 504.3820216655731, 502.10309958457947, 499.83669877052307, 497.5818009376526]\n"
          ]
        }
      ],
      "source": [
        "# Treinamento\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for context, target in ngrams:\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "        model.zero_grad()\n",
        "        log_probs = model(context_idxs)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "\n",
        "print(\"Losses:\", losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1EVJ5naAIK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd3e9ad-725a-4776-da52-3c30d207ff3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'forty', 'were', 'all', 'child', 'all', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child', 'child', 'thy', 'all', 'dig', 'child']\n"
          ]
        }
      ],
      "source": [
        "# Função de geração de texto\n",
        "\n",
        "print(model.generate([\"When\", \"forty\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculo da similaridade de duas palavras\n",
        "idx1 = torch.tensor([word_to_ix[\"thou\"]], dtype=torch.long)\n",
        "idx2 = torch.tensor([word_to_ix[\"thy\"]], dtype=torch.long)\n",
        "\n",
        "embedding1 = model.embeddings(idx1)\n",
        "embedding2 = model.embeddings(idx2)\n",
        "\n",
        "similarity = F.cosine_similarity(embedding1, embedding2, dim=1)\n",
        "print(similarity)\n",
        "\n",
        "# Thou e thy são ambos pronomes pessoais, e calculando a similaridade via cosseno, tem-se 0.3179,\n",
        "# o que indica que os vetores apontam a uma direção similar, porém não identica, caso contrário seria 1.\n",
        "\n",
        "idx1 = torch.tensor([word_to_ix[\"sunken\"]], dtype=torch.long)\n",
        "idx2 = torch.tensor([word_to_ix[\"child\"]], dtype=torch.long)\n",
        "\n",
        "embedding1 = model.embeddings(idx1)\n",
        "embedding2 = model.embeddings(idx2)\n",
        "\n",
        "similarity = F.cosine_similarity(embedding1, embedding2, dim=1)\n",
        "print(similarity)\n",
        "\n",
        "# Sunken e child são palavras que não possuem qualquer relação, seja a nivel de contexto ou significado, dessa forma,\n",
        "# a similaridade -0.6515 indica uma opsição dos vetores em um espaço vetorial."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVfSGsHX_0VK",
        "outputId": "58220062-c12c-4e0b-acf4-1a13fa207e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3179], grad_fn=<SumBackward1>)\n",
            "tensor([-0.6515], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Alterações de código\n",
        "\n",
        "# -> aumentar o tamanho do embedding\n",
        "# -> diminuicao do learning_rate, visto que 0.001 estava pequeno e o modelo não estava aprendendo.\n",
        "# -> aumento do numero de épocas, otimizando a loss\n",
        "\n",
        "EMBEDDING_DIM = 15\n",
        "\n",
        "# Treinamento\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(30):\n",
        "    total_loss = 0\n",
        "    for context, target in ngrams:\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "        model.zero_grad()\n",
        "        log_probs = model(context_idxs)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "\n",
        "print(\"Losses:\", losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhjB1mRWGcYC",
        "outputId": "300bb4d2-906d-4166-a869-139f5686c0a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses: [551.0401282310486, 380.2437974959612, 192.15577979385853, 68.46533136535436, 26.146793918916956, 13.064886529231444, 10.680896299891174, 9.628649304388091, 8.8035327012185, 8.251356885535643, 7.789733911398798, 7.419678502250463, 7.133399412850849, 6.87472845101729, 6.66947423259262, 6.478245869628154, 6.317645343602635, 6.175071881036274, 6.042119850288145, 5.932132281828672, 5.818120769690722, 5.723222768632695, 5.634200125874486, 5.554138747567777, 5.477720849448815, 5.4070484265103005, 5.353661676344927, 5.28708635084331, 5.226724117877893, 5.173188592947554]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de geração de texto\n",
        "\n",
        "print(model.generate([\"When\", \"forty\"]))"
      ],
      "metadata": {
        "id": "DFfVklX9Hw0X",
        "outputId": "eb895eb7-8e7c-4081-cda6-54a053a1bbbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'forty', 'winters', 'shall', 'besiege', 'thy', 'brow,', 'And', 'dig', 'deep', 'trenches', 'in', 'thy', \"beauty's\", 'use,', 'If', 'thou', 'couldst', 'answer', \"'This\", 'fair', 'child', 'of', 'mine', 'Shall', 'sum', 'my', 'count,', 'and', 'make', 'my', 'old', \"excuse,'\", 'Proving', 'his', 'beauty', 'by', 'succession', 'thine!', 'This', 'were', 'to', 'be', 'new', 'made', 'when', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.', 'thou', \"feel'st\", 'it', 'cold.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12p2tj-czmEH"
      },
      "source": [
        "### Atividades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK3egeQ3zpYa"
      },
      "source": [
        "__1 - Escreva a função generate da classe NGramLanguageModel.__\n",
        "\n",
        "__2 - Depois de treinar o modelo, gere uma sentença de 128 tokens.__\n",
        "\n",
        "__3 - Calcule e print a similaridade entre duas palavras. A similaridade resultante está correta? Justifique a sua resposta.__\n",
        "\n",
        "__4 - Proponha três alterações no código e demonstre que melhorou o desempenho do modelo.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUUyNF04BdyC"
      },
      "source": [
        "## Character-Level RNN Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beqhEWJqJAV3"
      },
      "outputs": [],
      "source": [
        "# RNN BASE\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input_combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(hidden)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYvtTwXIypmQ"
      },
      "outputs": [],
      "source": [
        "# def load_dataset\n",
        "# def preprocess_data\n",
        "# def tokenize_data\n",
        "# def create_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FryXi6d8zL6x"
      },
      "outputs": [],
      "source": [
        "# Parâmetros do modelo\n",
        "n_hidden = 128\n",
        "learning_rate = # Setar LR\n",
        "n_epochs = # Setar epoch\n",
        "print_every = # Setar log epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avgrvR5qKGgF"
      },
      "outputs": [],
      "source": [
        "rnn = RNN(vocab_len, n_hidden, vocab_len)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "def train(input_tensor, target_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "    rnn.zero_grad()\n",
        "    loss = 0\n",
        "    for i in range(input_tensor.size(0)):\n",
        "        output, hidden = rnn(input_tensor[i], hidden)\n",
        "        loss += criterion(output, target_tensor[i].unsqueeze(0))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item() / input_tensor.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVhiIarsKGdt"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, n_epochs + 1):\n",
        "    total_loss = 0\n",
        "    for start_idx in range(0, len(test_sentence) - max_sequence_len, max_sequence_len):\n",
        "        input_tensor, target_tensor = input_target_tensor(test_sentence, start_idx, max_sequence_len)\n",
        "        loss = train(input_tensor, target_tensor)\n",
        "        total_loss += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print(f'Epoch: {epoch}, Loss: {loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAAq1xab2Jy2"
      },
      "source": [
        "###  Atividades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuDCK43l2N6z"
      },
      "source": [
        "__1 - Escreva a função generate da classe RNN.__\n",
        "\n",
        "__2 - Escreva as funções de load_dataset, preprocess_data, tokenize_data e create_input.__\n",
        "\n",
        "__3 - Realize otimização de hiperparâmetros. Justifique a escolha dos hiperparâmetros otimizados e o espaço de busca definido.__\n",
        "\n",
        "__4 - Adicione uma Layer de Dropout na classe RNN. Treine o novo modelo e argumente sobre o impacto dessa alteração no modelo.__\n",
        "\n",
        "__5 - Adicione uma nova nn.Layer que recebe como input os vetores hidden e output combinados. Treine o novo modelo e argumente sobre o impacto dessa alteração no modelo.__\n",
        "\n",
        "__6 - Adicione uma função para printar uma geração de texto de no máximo 100 caracteres sempre que printar a loss do modelo.__\n",
        "\n",
        "__7 - Adicione uma função que calcula a perplexidade e printe com a loss.__\n",
        "\n",
        "__8 - Proponha três alterações no código e demonstre que melhorou o desempenho do modelo.__\n",
        "\n",
        "**Desafio**\n",
        "\n",
        "__1 - Desenvolva um modelo Word-Level utilizando LSTM.__\n",
        "- __Escrever a classe LSTM.__\n",
        "- __Escrever a função generate.__\n",
        "- __Otimizar o modelo.__\n",
        "- __Comparar com as outras abordagens acima__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BdUmCbw7V62"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}